{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Multi-Head Latent Attention\n",
        "\n",
        "Multi-Head Latent Attention (**MLA**) is the key to efficiency at scale. Recent models represent advancement in how they handle the attention mechanism. DeepSeek with 671B parameters (37B activated) needed innovative approach to maintain efficiency without compromising quality."
      ],
      "metadata": {
        "id": "UZruGKstLZdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLA is built upon Key-Value cache with a new flow for key value computation introducing **Down-projection** projecting the input embedding into a compressed latent space, **Storage** compressing representation in KV cache, and **Up-Projection** to reconstruct the full sized key and value matrices on the fly. With this we reduce the footprint and preserve the model quality.\n"
      ],
      "metadata": {
        "id": "MZVYVzRMShgZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vugtoEckFS2j"
      },
      "source": [
        "The **implementation** compresses the KV Cache with MLA, introduces RoPE injecting positional awareness with Rotary Positional Encoding (RoPE) and using MLA and RoPE with a decoupled architecture. RoPE encodes position directly into attention by rotating vectors in complex space preserving the relative distance betwen tokens regardless of the context length.\n",
        "\n",
        "Additional innovation is incorporated for position representation - the architecture combines MLA with a decoupled positional encoding system. Attention is split into parallel paths, content path (MLA) and positon path (RoPE) to parameter efficiency, training and scaling as each representation is specific."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L-hUjLVxzGQ"
      },
      "source": [
        "####**Working with the Latent Matrix**\n",
        "\n",
        "- The Query Matric calculation is standard, direct projection of input embeddings. $Q = X \\cdot W_q$\n",
        "- The Latent $KV$ Matrix $c_KV$ is the input embeddings projected down into the compressed latent space. The $c_KV$ is what we eventually cache.  $c_KV = X  \\cdot W_kv$\n",
        "- The Key matrix $K$ is no longer a direct projection of $X$, instead an up-projection of the of latent matrix, $c_KV$.  $c_KV \\cdot W_uk$. If we make the substitutions we see the full transformation from the original input  ($X \\cdot W_kv$) \\cdot $W_uk$ $K = c_KV \\cdot W_uk = X \\cdot W_kv \\cdot W_uk$\n",
        "- The value matrix V is also the up-projection of the same latent matrix $cKV$, and with substitutions we get ($X \\cdot W_kv$)$ \\cdot W_uv$ - $V = c_KV \\cdot W_uv = X \\cdot W_kv \\cdot W_uv$\n",
        "\n",
        "The absorption in getting the attention scores with substitutions get us into new definitions.\n",
        "\n",
        "**Attention Score** = $Q \\cdot K^T = X \\cdot (W_q \\cdot W_k^T) \\cdot (X \\times W_{kv})^T$\n",
        "\n",
        "($W_q \\cdot W_uk^T$) is fixed at training time and $(X \\cdot W_kv)^T$ is what get cached.\n",
        "\n",
        "**Context Vector Matrix** = Attention Scores ($Q \\cdot K^T$) Cached ($X \\cdot W_dkv$) Fixed-at-training ($W_uv \\cdot W_o$)\n",
        "\n",
        "Final **output projection layer** W_o: (Attention Weights) $\\cdot$ ( ($X \\cdot  W_d-kv) \\cdot W_uv$) -- **cached latent matrix**. The simplified process to get the attention scores with the original formula $Q \\cdot K^T$ effectively becomes:\n",
        "(Input $X$) $\\cdot$ (A fixed, pre-computed matrix) $\\cdot$ (The transpose of latent $c_KV$ matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''  Apply Rotary Positional Encoding, not part of embedding, and\n",
        "applied to Query and Key vectors '''\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, head_size, max_seq_len=2048):\n",
        "      super().__init__()\n",
        "      theta = 1.0 / (10000 ** torch.arange(0, head_size, 2).float()/head_size)\n",
        "      self.register_buffer('theta', theta)\n",
        "\n",
        "      positions = torch.arange(max_seq_len).float().unsqueeze(1)\n",
        "      frequencies = positions * self.theta.unsqueeze(0)\n",
        "      self.register_buffer(\n",
        "              'frequencies_complex',\n",
        "              torch.polar(torch.ones_like(frequencies), frequencies))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[2]\n",
        "        x_complex = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "        x_complex = torch.view_as_complex(x_complex)\n",
        "        frequencies_complex = self.frequencies_complex[:seq_len, :].\\\n",
        "                                                    unsqueeze(0).unsqueeze(0)\n",
        "        x_rotated = x_complex * frequencies_complex\n",
        "        x_rotated = torch.view_as_real(x_rotated)\n",
        "        x_rotated = x_rotated.flatten(3)\n",
        "        return x_rotated.type_as(x)"
      ],
      "metadata": {
        "id": "wzYsqZdpiwNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The full sota attention mechanism from DeepSeek,\n",
        "Multi-Head Attention (MLA) with Rotational Positional Encoding '''\n",
        "class MLAAttention(nn.Module):\n",
        "      def __init__(self, ds_model, num_heads, d_latent, d_rope, dropout=0.0,\n",
        "                   max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        assert ds_model % num_heads == 0, 'ds model divisible by num_heads'\n",
        "        self.ds_model = ds_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = ds_model // num_heads\n",
        "        self.d_latent = d_latent\n",
        "        self.d_rope = d_rope\n",
        "\n",
        "        self.W_query_content = nn.Linear(ds_model, ds_model)\n",
        "        self.W_dkv_content = nn.Linear(ds_model, d_latent)\n",
        "        self.W_uk_content = nn.Linear(d_latent, ds_model)\n",
        "        self.W_uv_content = nn.Linear(d_latent, ds_model)\n",
        "\n",
        "        self.W_k_pos = nn.Linear(ds_model, d_rope * num_heads)\n",
        "        self.W_q_pos = nn.Linear(ds_model, d_rope * num_heads)\n",
        "\n",
        "        self.rope = RoPE(d_rope, max_seq_len)\n",
        "\n",
        "        self.W_out_proj = nn.Linear(ds_model, ds_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(\n",
        "            torch.ones(1, 1, max_seq_len, max_seq_len), diagonal=1).bool())\n",
        "\n",
        "      def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        q_c = self.W_query_content(x).view(\n",
        "            batch_size, seq_len, self.num_heads, self.head_size).transpose(1,2)\n",
        "        c_kv = self.W_dkv_content(x)\n",
        "        k_c = self.W_uk_content(c_kv).view(\n",
        "            batch_size, seq_len, self.num_heads, self.head_size).transpose(1,2)\n",
        "        v_c = self.W_uv_content(c_kv).view(\n",
        "            batch_size, seq_len, self.num_heads, self.head_size).transpose(1,2)\n",
        "\n",
        "        q_r_unrotated = self.W_q_pos(x).view(\n",
        "            batch_size, seq_len, self.num_heads, self.head_size).transpose(1,2)\n",
        "        k_r_unrotated = self.W_k_pos(x).view(\n",
        "            batch_size, seq_len, self.num_heads, self.head_size).transpose(1,2)\n",
        "        q_r = self.rope(q_r_unrotated)\n",
        "        k_r = self.rope(k_r_unrotated)\n",
        "\n",
        "        content_scores = torch.matmul(\n",
        "            q_c, k_c.transpose(-2, -1)) / (self.head_size **0.5)\n",
        "        position_scores = torch.matmul(\n",
        "            q_r, k_r.transpose(-2, -1)) / (self.d_rope **0.5)\n",
        "        attn_scores = content_scores + position_scores\n",
        "        attn_scores = attn_scores.masked_fill(\n",
        "            self.mask[:, :, :seq_len, :seq_len], float('-inf'))\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vector = (attn_weights @ v_c).transpose(1, 2).contiguous().\\\n",
        "                            view(batch_size, seq_len, self.ds_model)\n",
        "        output = self.W_out_proj(context_vector)\n",
        "        return output"
      ],
      "metadata": {
        "id": "dOadbMkNiyou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_model = 512\n",
        "num_heads = 8\n",
        "d_latent = 128\n",
        "d_rope = 64\n",
        "batch_size = 4\n",
        "seq_len = 64\n",
        "\n",
        "deepseek_attn_layer = MLAAttention(ds_model, num_heads, d_latent, d_rope)\n",
        "x = torch.randn(batch_size, seq_len, ds_model)\n",
        "output = deepseek_attn_layer(x)\n",
        "print('Deep Seek Layer')\n",
        "print(f'input shape: {x.shape}')\n",
        "print(f'output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "IZKT7IB8kGhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}