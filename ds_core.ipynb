{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###DeepSeek\n",
        "\n",
        "**Build a mini DeepSeek model** focusing on key technical innovations to give a deep, practical understanding of modern LLM architecture and training. This is a hands-on process to implement and adapt the state-of-the-art AI techniques."
      ],
      "metadata": {
        "id": "bQhehMWl_Hig"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBagGwNILUJM"
      },
      "source": [
        "###DS Core Architecture\n",
        "\n",
        "The architecture is divided in four stages for design and imlementation.\n",
        "- **Key-Value Cache**: Establishing memory efficiency through Multi-Query (MQA) and Grouped-Query Attention (GQA).\n",
        "- DS Core Architecture with **MultiHead Latent Attention (MLA)** and **Mixture of Models (MoE)**\n",
        "- Foundational Model with **Multi Token Prediction (MTP), FP8, Dual Pipe Parallelism**\n",
        "- **Post Training** - Reasoning and Distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Key-Value Cache, MQA, GQA**\n",
        "\n",
        "KV Cache serves as the foundation for all subseqeunt attention optimizations in DS models. A design technique to achieve the remarkable balance of quality and efficiency at scale. It reduces the computational cost of autoregressive generation, and in memory-computation tradeoff exemplifies an essential engineering principle. MQA and GQA build directly on the KV cache foundation further optimizing memory usage while maintaining quality."
      ],
      "metadata": {
        "id": "yg0TloREJZMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**The Multi-Head Latent Attention** (**MLA**)\n",
        "\n",
        "**MLA** is a significant architecture innovation in the attention mechanism design addressing the challenges with memory and preserving the model quality. MLA compresses for storage and decompreses for use, decouples content-position architecture and trades the computation (up-projection) for decreased memory usage. These innovations enable technical sepcifications (671B parameters, 128K context window), and change what is possible with LLMs including extended reasoning over long documents, improved memory retrieval and cost-effective deployment."
      ],
      "metadata": {
        "id": "2XFm2e8ySsl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Mixture of Experts**\n",
        "\n",
        "**Mixture of Experts** (**MoE**) address the challenge with large language models reaching into hundreds of billions that present significant challenges with the inferece,\n",
        "\n",
        "MOE design employs multiple specialized neural networks (experts) and a routing mechanism to direct different inputs to appropriate experts.  MOE approach allows to scale to enormous parameters and keep the computational costs manageable. **DeepSeek** **MoE** architecture includes **Shared experts** active for all tokens, **Routed experts** selectively acivated based on tokens and Load balancing. **What makes DeepSeek MoE different** is the hybrid expert design as a combination of shared generalists and specialized routed experts that allow for an effective balanced computations, no auxiliary loss for the router to focus on performance, and the expert specialization to allow for more specialization based on token patterns."
      ],
      "metadata": {
        "id": "nxD-DiLcAyjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall the DeepSeek MoE approach gives meaningful benefits that would scale the model size, better parameter efficiency, faster training and inference, and optimization."
      ],
      "metadata": {
        "id": "YPkzh8qd7KUN"
      }
    }
  ]
}