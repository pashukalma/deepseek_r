{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###DeepSeek\n",
        "\n",
        "Objective: **Build a mini DeepSeek model** focusing on key technical innovations to give a deep, practical understanding of modern LLM architecture and training. This is a hands-on process to imlement and adapt the state-of-the-art AI techniques."
      ],
      "metadata": {
        "id": "bQhehMWl_Hig"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBagGwNILUJM"
      },
      "source": [
        "###DS Core Architecture\n",
        "\n",
        "The architecture is divided in four stages for design and imlementation.\n",
        "- **Key-Value Cache**: Establishing memory efficiency through Multi-Query (MQA) and Grouped-Query Attention (GQA).\n",
        "- DS Core Architecture with **MultiHead Latent Attention (MLA)** and **Mixture of Models (MoE)**\n",
        "- Foundational Model with **Multi Token Prediction (MTP), FP8, Dual Pipe Parallelism**\n",
        "- **Post Training** - Reasoning and Distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Key-Value Cache, MQA, GQA**\n",
        "\n",
        "KV Cache serves as the foundation for all subseqeunt attention optimizations in DS models. It is a design technique to achieve the remarkable balance of quality and efficiency at scale. It reduces the computational cost of autoregressive generation, and in memory-computation tradeoff exemplifies an essential engineering principle. MQA and GQA build directly on the KV cache foundation further optimizing memory usage while maintaining quality."
      ],
      "metadata": {
        "id": "yg0TloREJZMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Multi-Head Latent Attention** (**MLA**)\n",
        "\n",
        "**MLA** is a significant architecture innovation in the attention mechanism design addressing the challenges with memory and preserving the model quality. MLA compresses for storage and decompreses for use, decouples content-position architecture and trades the computation (up-projection) for decreased memory usage.\n",
        "\n",
        "These innovations enable technical sepcifications (671B parameters, 128K context window), and change what is possible with LLms including extended reasoning over long documents, improved memory retrieval and cost-effective deployment."
      ],
      "metadata": {
        "id": "2XFm2e8ySsl_"
      }
    }
  ]
}