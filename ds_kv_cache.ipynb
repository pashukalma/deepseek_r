{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Inference with KV Cache"
      ],
      "metadata": {
        "id": "AZbRxf-0_2GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KV Cache is a fundamental optimization technique that addresses performance in deploying LLMs like DeepSeek.\n",
        "\n",
        "In autoregressive generation each new token attention computation across previous tokens in the sequence without optimization would lead to quadratically increasing computation time, redundancy in computation and prohibitive memory and computational costs for practical applications.\n",
        "\n",
        "The Key-value cache solves this dramatically reducing the computations. It is the foundational technique for addressing inference in transformer models like DeepSeek or Llama.\n",
        "\n",
        "With key-value cache improving inference, DeepSeek further optimizes with advanced attention mechanism to handle massive scale efficiently"
      ],
      "metadata": {
        "id": "tTtAH7kCAU2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KV Cache**: key and Value projections can be computed once and reused.\n",
        "\n",
        "\n",
        "```\n",
        "Algorithm\n",
        "  Initial computation -compute Q, K, V\n",
        "  Storage -save K, V tensors in cache\n",
        "  Subsequent tokens -\n",
        "      compute Q, K, V only for new tokens\n",
        "      retrieve previous K, V from cache\n",
        "      concatenate the new K, V with new cached ones\n",
        "      store the expanded K, V in cache\n",
        "Complexity from O(n^2) to O(n) per tojen, n being the sequence length\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vxo2gCKBDF06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head Attention** (MQA): **MQA** uses a single shared K, V projection across all attention heads, and maintains a separate Q projections for each head.\n",
        "\n",
        "Key variable here is the number of attention heads as the total memory scales linearly with the number of heads. DeepSeek-v3 (671b) HAS 128 attention heads, the MQA reduces the KV cache size by a factor of 128, from 400GB down to just over 3GB. While MQA is a solution seen as memory-first approach, fundamentally it compromises the core strength of multi-head design. The primary efficiency gain comes from the reduced size of the Key and Value caches as we cache tensors of shape (batch_size, num_heads, seq_len, head_size), reducing drastically the memory.\n",
        "\n",
        "**- KV Cache Size** in MQA: $l * b * 1 * h * s * 2*2$"
      ],
      "metadata": {
        "id": "fa0u2_r3ECcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "In the Multi-Query Layer Attention the query projection maps to full model\n",
        "dimension. Key, value projections map to just a single head dimension.\n",
        "Use the repeat function to duplicate the single key and value for all query.\n",
        "'''\n",
        "class MQA(torch.nn.Module):\n",
        "    def __init__(self, ds_model, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert ds_model % num_heads == 0, 'ds model divisible by num_heads'\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = ds_model // num_heads\n",
        "\n",
        "        ''' the Query projection remains the same as the standard MHA '''\n",
        "        self.W_query_proj = nn.Linear(ds_model, ds_model)\n",
        "        ''' the Key and Value projections are now single, shared linear layers,\n",
        "        projecting down to the dimension of a single head (head_size) '''\n",
        "        self.W_key_proj = nn.Linear(ds_model, self.head_size)\n",
        "        self.W_value_proj = nn.Linear(ds_model, self.head_size)\n",
        "        self.W_out_proj = nn.Linear(ds_model, ds_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(1, 1, 1024, 1024)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        q = self.W_query_proj(x).view(batch_size, seq_len,\n",
        "                              self.num_heads, self.head_size).transpose(1, 2)\n",
        "        k = self.W_key_proj(x).view(batch_size, seq_len, 1,\n",
        "                              self.head_size).transpose(1, 2)\n",
        "        v = self.W_value_proj(x).view(batch_size, seq_len, 1,\n",
        "                              self.head_size).transpose(1, 2)\n",
        "\n",
        "        ''' the single Key and Value tensors are repeated or broadcast\n",
        "        to match the number of query heads '''\n",
        "        k = k.repeat(1, self.num_heads, 1, 1)\n",
        "        v = v.repeat(1, self.num_heads, 1, 1)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n",
        "                                self.head_size**.5) #(q @ k.transpose(-2, -1))\n",
        "\n",
        "        attn_scores = attn_scores.masked_fill(\n",
        "            self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context_vector = (attn_weights @ v).transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, -1)\n",
        "        output = self.W_out_proj(context_vector)\n",
        "        return output\n",
        "\n",
        "ds_model = 512\n",
        "num_heads = 8\n",
        "batch_size = 4\n",
        "seq_len = 64\n",
        "\n",
        "mqa_layer = MQA(ds_model, num_heads)\n",
        "x = torch.randn(batch_size, seq_len, ds_model)\n",
        "output = mqa_layer(x)\n",
        "print(x.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "pztjvaSpi97H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grouped-Query Attention** (GQA); **GQA** groups the attention heads into clusters, 4-8 for 32 heads, with each group sharing the same K, V projections, and queries remaining separate for each head.\n",
        "\n",
        "The trade-off sacrificing model expressivity for memory efficiency is not ideal and led to seek a more balanced approach, a technique that could offer substantial memory savings without dismantling the power of multi-head design. It is a pragmatic compromise between MHA and MQA.\n",
        "\n",
        "The core idea of Grouped-Query Attention is that instead of forcing all attention heads to share the same Key and Value matrices, we create goups of attention heads and only share the Keys and Values within these groups.\n",
        "\n",
        "**- KV Cache Size in GQA**: $l * b * g * h * s * 2*2$"
      ],
      "metadata": {
        "id": "siJ9V6irEhOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' In the Group-Query Attention, the Query projection maps to the full\n",
        "model dimension. the key and value projections map to num_groups*head_size.\n",
        "Use repeat interleave to match each key and value group with its corresponding\n",
        "query heads. '''\n",
        "class GQA(torch.nn.Module):\n",
        "    def __init__(self, ds_model, num_heads, num_groups, dropout=0.0,\n",
        "                 max_seq_len: int=0):\n",
        "        super().__init__()\n",
        "        assert ds_model % num_heads == 0, 'ds model divisible by num_heads'\n",
        "        assert num_heads % num_groups == 0, 'num_heads divisible by num_groups'\n",
        "\n",
        "        self.ds_model = ds_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_groups = num_groups\n",
        "        self.head_size = ds_model // num_heads\n",
        "\n",
        "        self.W_query_proj = nn.Linear(ds_model, ds_model)\n",
        "        ''' Instead of creating a single projection (head_size-number of heads)\n",
        "         we create num_groups projections '''\n",
        "        self.W_key_proj = nn.Linear(ds_model, self.head_size)\n",
        "        self.W_value_proj = nn.Linear(ds_model, self.head_size)\n",
        "        self.W_out_proj = nn.Linear(ds_model, ds_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(1, 1, 1024, 1024)))\n",
        "\n",
        "    def _get_causal_mask(self, seq_len, device):\n",
        "            if self.causal_mask is not None and self.causal_mask.size[-1] >=seq_len:\n",
        "                return self.causal_mask[:, :, :seq_len, :seq_len]\n",
        "            return torch.triu(\n",
        "                torch.ones(1, 1, seq_len, seq_len), diagonal=1).to(device)\n",
        "\n",
        "    def _register_mask_buffer(self, max_seq_len):\n",
        "            if max_seq_len >0:\n",
        "                mask = torch.triu(torch.ones(\n",
        "                    1, 1, max_seq_len, max_seq_len), dtype=torch.bool).to(device)\n",
        "                self.register_buffer('causal_mask', mask, persistence=False)\n",
        "            else:\n",
        "                self.causal_mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "            batch_size, seq_len, _ = x.shape()\n",
        "\n",
        "            q = self.W_query_proj(x).view(\n",
        "              batch_size, seq_len, self.num_groups, self.head_size).transpose(1, 2)\n",
        "            ''' the input is projected and reshaped into\n",
        "            num_groups distinct Key and Value groups '''\n",
        "            k = self.W_key_proj(x).view(\n",
        "              batch_size, seq_len, self.num_groups, self.head_size).transpose(1, 2)\n",
        "            v = self.W_value_proj(x).view(\n",
        "              batch_size, seq_len, self.num_groups, self.head_size).transpose(1, 2)\n",
        "\n",
        "            heads_per_group = self.num_heads // self.num_groups\n",
        "            ''' repeat_interleave broadcasts the K/V groups to query heads,\n",
        "            each of the num_groups of Keys and Values is shared across head\n",
        "            per group queries '''\n",
        "            k = k.repeat_interleave(heads_per_group, dim=2)\n",
        "            v = v.repeat_interleave(heads_per_group, dim=2)\n",
        "\n",
        "            attn_scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "            causal_mask = self._get_causal_mask(seq_len, x.device)\n",
        "\n",
        "            attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
        "            attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "            context = (attn_weights @ v).transpose(1, 2).contiguous().view(\n",
        "                batch_size, seq_len, self.ds_model)\n",
        "            return self.W_out_proj(context)"
      ],
      "metadata": {
        "id": "gbo6xvwfi_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_model = 512\n",
        "num_heads = 32\n",
        "num_groups = 4\n",
        "batch_size = 4\n",
        "seq_len = 64\n",
        "\n",
        "''' By changing the num_groups, we can move seamlessly from MQA like behavior\n",
        "(num_groups=1) to MHA-like behavior (num_groups=head_size, number-of-heads) '''\n",
        "gqa_layer = GQA(ds_model, num_heads, num_groups)\n",
        "x = torch.randn(batch_size, seq_len, ds_model)\n",
        "output = mqa_layer(x)\n",
        "print(x.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "eS-OO42Hjr7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}