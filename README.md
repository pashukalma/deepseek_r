# deepseek_r
build, train deepseek

Build a mini DeepSeek model focusing on key technical innovations to give a deep, practical understanding of modern LLM architecture and training. This is a hands-on process to implement and adapt the state-of-the-art AI techniques.


The repository is meant to provide a deep dive into the **DeepSeek-V3** architecture, specifically focusing on its two most significant innovations: Multi-Head Latent Attention (MLA) and DeepSeek-MoE. 

**Core Architecture** (ds_core.ipynb)
Four Stages of Design: From KV Cache foundations to Reasoning and Distillation.

Multi-Head Latent Attention (**MLA**) (ds_mla.ipynb)

Mixture of Experts (**MoE**) Foundations (ds_moe_standard.ipynb, ds_moe.ipynb)

**DeepSeek-MoE**: Advanced Implementation (ds_moe_deepseek.ipynb)
**Training workflow** from tokenization, data processing, mixed precision to benchmarks and parameter efficiency metrics.